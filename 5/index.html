<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<title>Neural Radiance Field — CS180/280A Project 4</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />

<style>
:root{
  --ink:#1b2a41;
  --muted:#557;
  --bg:#f7fbff;
  --card:#ffffff;
  --accent:#0066cc;
  --accent2:#004c99;
  --soft:#e8f2ff;
  --rule:#ccd9ea;
}
body{
  margin:32px;
  font-family: "Segoe UI", Arial, sans-serif;
  background:var(--bg);
  color:var(--ink);
  line-height:1.65;
}
h1{
  text-align:center;
  font-size:2.3rem;
  margin-bottom:12px;
}
h2{
  margin-top:44px;
  border-left:8px solid var(--accent);
  background:linear-gradient(to right,var(--soft),#fff);
  padding:8px 12px;
  border-radius:6px;
  font-size:1.7rem;
}
h3{ margin-top:24px; font-size:1.3rem; }
figure{
  margin:0;
  padding:14px;
  background:var(--card);
  border:1px solid var(--rule);
  border-radius:10px;
  box-shadow:1px 2px 6px rgba(0,0,0,.05);
}
figcaption{
  margin-top:8px;
  color:var(--muted);
  font-size:.95rem;
}
.big img{ width:100%; max-width:900px; border-radius:10px; }
.grid2x2{
  display:grid;
  grid-template-columns:1fr 1fr;
  gap:20px;
}
@media(max-width:900px){
  .grid2x2{ grid-template-columns:1fr; }
}
</style>
</head>

<body>

<h1>Neural Radiance Field — CS180/280A Project 4</h1>

<p>
This project explores Neural Radiance Fields (NeRFs) through several stages:
camera calibration, 2D neural fields, multi-view 3D reconstruction, and training NeRFs on a self-captured dataset.
We begin by calibrating a camera using ArUco markers, fitting a neural field to a single 2D image,
and then scaling the system to full volumetric NeRF training using the Lego dataset.
Each step builds toward the full pipeline of coordinate-based scene representation and novel-view synthesis.
</p>

<!-- ====================================================== -->
<h2>Part 0 — Camera Calibration & 3D Capture</h2>

<h3>0.1 Calibrating the Camera</h3>
<p>
I captured multiple calibration images of a printed ArUco board and detected the markers using OpenCV’s 4×4 dictionary.
By pairing the detected 2D corner locations with known 3D coordinates,
cv2.calibrateCamera recovered the intrinsic matrix and distortion parameters.
These intrinsics ensure all later ray geometry and NeRF training operate under a correct pinhole-camera model.
</p>

<h3>0.2 Capturing Object Scan</h3>
<p>
I photographed my chosen object from 30–50 viewpoints while keeping the same zoom and lighting.
Each image contains an ArUco tag, enabling accurate pose recovery.
After undistortion, these images form a consistent multi-view dataset required for 3D supervision in NeRF.
</p>

<h3>0.3 Estimating Camera Pose</h3>
<p>
Using cv2.solvePnP, I estimated the camera pose for each object image.
The returned rotation and translation describe the camera relative to the ArUco tag;
I invert this to obtain camera-to-world matrices for NeRF.
Visualizing these poses in Viser confirmed a clean orbit around the object.
</p>

<div class="grid2x2">
  <figure class="big">
    <img src="./media/p0_camera_frustums_view1.png">
    <figcaption>Camera frustums visualization (View 1).</figcaption>
  </figure>
  <figure class="big">
    <img src="./media/p0_camera_frustums_view2.png">
    <figcaption>Camera frustums visualization (View 2).</figcaption>
  </figure>
</div>

<!-- ====================================================== -->
<h2>Part 1 — Neural Field on a 2D Image</h2>

<h3>1.1 Training on a Provided Image</h3>
<p>
I implemented a 2D neural field: a positional-encoding MLP mapping normalized pixel coordinates to RGB.
The model learns the entire image as a continuous function.
Early iterations produce blurry approximations, but the reconstruction sharpens as high-frequency components are learned.
</p>

<figure class="big">
  <img src="./media/p1_provided_final_reconstruction.png">
  <figcaption>Final reconstruction — provided image.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p1_provided_training_progression.png">
  <figcaption>Training progression — provided image.</figcaption>
</figure>

<h3>1.2 Training on My Own Image</h3>
<p>
Applying the pipeline to my own image produced similar behavior:
the network gradually learned edges, colors, and textures.
Positional encoding allowed representing fine detail and prevented blurry convergence.
</p>

<figure class="big">
  <img src="./media/p1_mine_final_reconstruction.png">
  <figcaption>Final reconstruction — my image.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p1_mine_training_progression.png">
  <figcaption>Training progression — my image.</figcaption>
</figure>

<h3>1.3 Hyperparameter Study (Frequency × Width)</h3>
<p>
I trained four models varying positional-encoding frequency and MLP width.
Higher frequencies allow sharper details but require more capacity to avoid aliasing.
Wider networks converge faster and achieve higher PSNR.
Below is a 2×2 comparison grid.
</p>

<figure class="big">
  <img src="./media/p1_hyperparam_grid.png">
  <figcaption>Final results for (L₁,W₁), (L₁,W₂), (L₂,W₁), (L₂,W₂).</figcaption>
</figure>

<h3>1.4 PSNR Curve</h3>
<p>
The curve below shows reconstruction PSNR during training.
It increases smoothly as the network learns high-frequency structure and stabilizes near convergence.
</p>

<figure class="big">
  <img src="./media/p1_psnr_curve.png">
  <figcaption>PSNR curve for training on provided image.</figcaption>
</figure>

<!-- ====================================================== -->
<h2>Part 2 — Neural Radiance Field (Lego)</h2>

<h3>2.1 & 2.2 Rays and Sampling</h3>
<p>
I implemented pixel-to-camera conversion, camera-to-world transformation, and ray construction.
Each pixel defines a ray, and each ray is discretized into 3D sample points between near/far bounds.
Stratified perturbation ensures robust coverage of the 3D space and prevents overfitting to fixed sample depths.
</p>

<h3>2.3 Visualization of Cameras, Rays, and Samples</h3>
<p>
Using Viser, I visualized camera frustums, sampled rays, and 3D sample points.
This confirmed correct ray directions, pixel-center offset handling, and proper world-space placement.
</p>

<div class="grid2x2">
  <figure class="big">
    <img src="./media/p2_rays_view1.png">
    <figcaption>Rays + samples visualization (View 1).</figcaption>
  </figure>
  <figure class="big">
    <img src="./media/p2_rays_view2.png">
    <figcaption>Rays + samples visualization (View 2).</figcaption>
  </figure>
</div>

<h3>2.4 NeRF Architecture</h3>
<p>
The NeRF model includes:
positional encoding for 3D coordinates and view directions,
a deep MLP with skip connection,
and separate branches for predicting density and view-dependent color.
This structure allows modeling geometry, occlusions, and specularities.
</p>

<figure class="big">
  <img src="./media/p2_architecture_diagram.png">
  <figcaption>NeRF MLP architecture.</figcaption>
</figure>

<h3>2.5 Volume Rendering & Training Results</h3>
<p>
I implemented the volume-rendering equation using accumulated transmittance and weighted color integration.
The model was trained on the Lego dataset, and the figures below show the training progression,
validation PSNR curve, and the final spherical render from the test camera path.
</p>

<figure class="big">
  <img src="./media/p2_training_progression.png">
  <figcaption>Training progression — Lego dataset.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p2_validation_psnr_curve.png">
  <figcaption>Validation PSNR curve.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p2_lego_spherical.gif">
  <figcaption>Spherical novel-view rendering of Lego.</figcaption>
</figure>

<!-- ====================================================== -->
<h2>Part 2.6 — NeRF on My Own Object</h2>

<h3>My Object</h3>
<p>
Below is the object I captured and used to train a NeRF.
This image serves as the reference appearance for the novel-view synthesis.
</p>

<figure class="big">
  <img src="./media/p26_my_object_reference.JPG">
  <figcaption>Reference image of my object.</figcaption>
</figure>

<h3>Training & Results</h3>
<p>
Training required adjusting near/far bounds and increasing samples per ray due to scale and lighting differences.
Below are the training-loss curve, an intermediate render,
and the final 360° novel-view GIF generated by my NeRF model.
</p>

<figure class="big">
  <img src="./media/p26_training_loss_curve.png">
  <figcaption>Training loss curve.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p26_intermediate_render.png">
  <figcaption>Intermediate render during training.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p26_final_novelview.gif">
  <figcaption>Final spherical rendering of my object.</figcaption>
</figure>

</body>
</html>
