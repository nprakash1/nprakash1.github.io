<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<title>Neural Radiance Field — CS180/280A Project 4</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />

<style>
:root{
  --ink:#1b2a41;
  --muted:#557;
  --bg:#f7fbff;
  --card:#ffffff;
  --accent:#0066cc;
  --accent2:#004c99;
  --soft:#e8f2ff;
  --rule:#ccd9ea;
}
body{
  margin:32px;
  font-family: "Segoe UI", Arial, sans-serif;
  background:var(--bg);
  color:var(--ink);
  line-height:1.65;
}
h1{
  text-align:center;
  font-size:2.3rem;
  margin-bottom:12px;
}
h2{
  margin-top:44px;
  border-left:8px solid var(--accent);
  background:linear-gradient(to right,var(--soft),#fff);
  padding:8px 12px;
  border-radius:6px;
  font-size:1.7rem;
}
h3{ margin-top:24px; font-size:1.3rem; }
figure{
  margin:0;
  padding:14px;
  background:var(--card);
  border:1px solid var(--rule);
  border-radius:10px;
  box-shadow:1px 2px 6px rgba(0,0,0,.05);
}
figcaption{
  margin-top:8px;
  color:var(--muted);
  font-size:.95rem;
}
.big img{ width:100%; max-width:900px; border-radius:10px; }
.grid2x2{
  display:grid;
  grid-template-columns:1fr 1fr;
  gap:20px;
}
@media(max-width:900px){
  .grid2x2{ grid-template-columns:1fr; }
}
</style>
</head>

<body>

<h1>Neural Radiance Field — CS180/280A Project 4</h1>

<p>
This project explores Neural Radiance Fields (NeRFs) through several stages:
camera calibration, 2D neural fields, multi-view 3D reconstruction, and training NeRFs on a self-captured dataset.
We begin by calibrating a camera using ArUco markers, fitting a neural field to a single 2D image,
and then scaling the system to full volumetric NeRF training using the Lego dataset.
Each step builds toward the full pipeline of coordinate-based scene representation and novel-view synthesis.
</p>

<!-- ====================================================== -->
<h2>Part 0 — Camera Calibration & 3D Capture</h2>

<h3>0.1 Calibrating the Camera</h3>
<p>
I captured multiple calibration images of a printed ArUco board and detected the markers using OpenCV’s 4×4 dictionary.
By pairing the detected 2D corner locations with known 3D coordinates,
cv2.calibrateCamera recovered the intrinsic matrix and distortion parameters.
These intrinsics ensure all later ray geometry and NeRF training operate under a correct pinhole-camera model.
</p>

<h3>0.2 Capturing Object Scan</h3>
<p>
I photographed my chosen object from 30–50 viewpoints while keeping the same zoom and lighting.
Each image contains an ArUco tag, enabling accurate pose recovery.
After undistortion, these images form a consistent multi-view dataset required for 3D supervision in NeRF.
</p>

<h3>0.3 Estimating Camera Pose</h3>
<p>
Using cv2.solvePnP, I estimated the camera pose for each object image.
The returned rotation and translation describe the camera relative to the ArUco tag;
I invert this to obtain camera-to-world matrices for NeRF.
Visualizing these poses in Viser confirmed a clean orbit around the object.
</p>

<div class="grid2x2">
  <figure class="big">
    <img src="./media/p0_camera_frustums_view1.png">
    <figcaption>Camera frustums visualization (View 1).</figcaption>
  </figure>
  <figure class="big">
    <img src="./media/p0_camera_frustums_view2.png">
    <figcaption>Camera frustums visualization (View 2).</figcaption>
  </figure>
</div>

<!-- ====================================================== -->
<h2>Part 1 — Neural Field on a 2D Image</h2>

<h3>1.1 Training on a Provided Image</h3>
<p> For the provided image, I trained a coordinate-based neural field implemented as an MLP that maps normalized (x, y) pixel coordinates directly to RGB values. I used sinusoidal positional encoding with L=10, which takes the 2 input coordinates and expands them into a 42-dimensional vector by concatenating the raw (x, y) values with 20 sine features and 20 cosine features. This encoded vector is fed into a 4-layer MLP consisting of three hidden layers of width 256 with ReLU activations, followed by a final linear layer that outputs 3 values and a Sigmoid to keep colors in the range [0, 1]. 
  The model was optimized using Adam with a learning rate of 1e-2 for 2000 iterations while randomly sampling 10k pixels per step. 
</p>

<figure class="big">
  <img src="./media/p1_provided_final_reconstruction.png">
  <figcaption>Final reconstruction — provided image.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p1_provided_training_progression.png">
  <figcaption>Training progression — provided image.</figcaption>
</figure>

<h3>1.2 Training on My Own Image</h3>
<p>
Applying the pipeline to my own image produced similar behavior:
the network gradually learned edges, colors, and textures.
Positional encoding allowed representing fine detail and prevented blurry convergence.
</p>

<figure class="big">
  <img src="./media/p1_mine_final_reconstruction.png">
  <figcaption>Final reconstruction — my image.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p1_mine_training_progression.png">
  <figcaption>Training progression — my image.</figcaption>
</figure>

<h3>1.3 Hyperparameter Study (Frequency × Width)</h3>
<p>
I trained four models varying positional-encoding frequency and MLP width.
Higher frequencies allow sharper details but require more capacity to avoid aliasing.
Wider networks converge faster and achieve higher PSNR.
Below is a 2×2 comparison grid.
</p>

<figure class="big">
  <img src="./media/p1_hyperparam_grid.png">
  <figcaption>Final results for (L₁,W₁), (L₁,W₂), (L₂,W₁), (L₂,W₂).</figcaption>
</figure>

<h3>1.4 PSNR Curve</h3>
<p>
The curve below shows reconstruction PSNR during training.
It increases smoothly as the network learns high-frequency structure and stabilizes near convergence.
</p>

<figure class="big">
  <img src="./media/p1_psnr_curve.png">
  <figcaption>PSNR curve for training on provided image.</figcaption>
</figure>

<!-- ====================================================== -->
<h2>Part 2 — Neural Radiance Field (Lego)</h2>

<h3>2.1 & 2.2 Rays and Sampling</h3>
<p>
To generate training rays, I implemented three key functions: pixel_to_camera, transform(c2w, x_c), and pixel_to_ray.
pixel_to_camera inverts the pinhole projection by unprojecting each pixel (u,v) into a 3D direction in camera space using the intrinsics 
K.transform(c2w, x_c) then maps these points into world coordinates using the camera-to-world matrix. With these pieces, pixel_to_ray creates each ray by extracting the camera origin from c2w and normalizing the world-space direction of the unprojected point.
Finally, I apply stratified sampling along each ray to generate perturbed depth samples, ensuring smooth coverage of the scene and preventing overfitting to fixed sample depths.
</p>

<h3>2.3 Visualization of Cameras, Rays, and Samples</h3>
<p>
Using Viser, I visualized camera frustums, sampled rays, and 3D sample points.
This confirmed correct ray directions, pixel-center offset handling, and proper world-space placement.
</p>

<div class="grid2x2">
  <figure class="big">
    <img src="./media/p2_rays_view1.png">
    <figcaption>Rays + samples visualization (View 1).</figcaption>
  </figure>
  <figure class="big">
    <img src="./media/p2_rays_view2.png">
    <figcaption>Rays + samples visualization (View 2).</figcaption>
  </figure>
</div>

<h3>2.4 NeRF Architecture</h3>
<p>
The NeRF model includes:
positional encoding for 3D coordinates and view directions,
a deep MLP with skip connection,
and separate branches for predicting density and view-dependent color.
This structure allows modeling geometry, occlusions, and specularities.
</p>

<figure class="big">
  <img src="./media/p2_architecture_diagram.png">
  <figcaption>NeRF MLP architecture.</figcaption>
</figure>

<h3>2.5 Volume Rendering & Training Results</h3>
<p>
I implemented the volume-rendering function by taking the density and color predicted at each sampled point on a ray and turning them into a probability that the ray “stops” at that point.
For every sample, the algorithm first computes how much light from earlier samples is still passing through, which is called the transmittance.
Then it computes how much that specific sample should contribute, based on its density (how solid it is) and its predicted color.
The final pixel color is the sum of all these contributions, where closer, denser samples naturally block out the ones behind them.
The model was trained on the Lego dataset, and the figures below show the training progression,
validation PSNR curve, and the final spherical render from the test camera path.
</p>

<figure class="big">
  <img src="./media/p2_training_progression.png">
  <figcaption>Training progression — Lego dataset.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p2_validation_psnr_curve.png">
  <figcaption>Validation PSNR curve.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p2_lego_spherical.gif">
  <figcaption>Spherical novel-view rendering of Lego.</figcaption>
</figure>

<!-- ====================================================== -->
<h2>Part 2.6 — NeRF on My Own Object</h2>

<h3>My Object</h3>
<p>
Below is the object I captured and used to train a NeRF.
This image serves as the reference appearance for the novel-view synthesis.
</p>

<figure class="big">
  <img src="./media/p26_my_object_reference.JPG">
  <figcaption>Reference image of my object.</figcaption>
</figure>

<h3>Training & Results</h3>
<p>
Training required adjusting near/far bounds and increasing samples per ray due to scale and lighting differences.
Below are the training-loss curve, an intermediate render,
and the final 360° novel-view GIF generated by my NeRF model.
</p>

<figure class="big">
  <img src="./media/p26_training_loss_curve.png">
  <figcaption>Training loss curve.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p26_intermediate_render.png">
  <figcaption>Intermediate render during training.</figcaption>
</figure>

<figure class="big">
  <img src="./media/p26_final_novelview.gif">
  <figcaption>Final spherical rendering of my object.</figcaption>
</figure>

</body>
</html>
